---
title: "Job Attrition and Retention"
author: "Julia Layne"
date: "4/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




## Intro

## Executive summary

Hiring and training employees only to see them walk out the door is expensive. DDSAnalytics would like to give companies the foresight in predicting employee attrition. In this analysis we will identify potential characteristics of attrition and estimate the salary of employees. We found that Overtime was disproportionate in employees that left the company. Lower Job Levels and Job Satifaction also contritbute to Attrition. Ensuring employees have a way to go up in level could be one way to mitigate Attrition. Considering the disproportionate amount of Overtime in those who chose to leave, it is important to pay attention to the job satisfaction of lower level employees who may be new to a position. 

## Introduction
This analysis will begin with a review of the data from 870 employees used to predict the outcomes for the provided competition set. First we will look at the top three fields in each prediction section: Attrition and Salary (MonthlyRate). We scaled numeric values for comparison. K-Nearest neighbors is used for the Attrition Model looking at: Overtime, Job Satisfaction, and Job Level. For the Salary, we estimated this using Linear Regression looking at: Age, Job Satisfaction, and Job Level.


## Links to supporting material

### Presentation
[Presentation Available](https://youtu.be/jEbQSMZb7KY)

## Initial Data
Reading in the data for training
```{r}
jobs = read.csv("CaseStudy2-data.csv",header = TRUE)
```

### A Summary of the Data
We found no missing values in the dataset. This allows us to use every row in considering which fields to consider in our analysis.
```{r}
#Find out which columns have missing values
missing <- colSums(is.na(jobs))>0
missing
```

An overview of the range of values in fields.
```{r}
summary(jobs) 
originalJobs <- jobs
```

### Import Libraries
R Libraries we will use for our analysis
```{r echo = FALSE,warning=FALSE}
#install.packages("GGally")
library(tidyverse)
library(reshape2)
library(GGally)
library(ggthemes)
library(caret)
library(mvtnorm)
library(class)
library(e1071)
```

### The Best Predictors found for both Models
For Attrition this was Overtime, Job Satisfaction, and Job Level
For Salary, we switched Overtime for Age. 
```{r warning=FALSE,echo=FALSE}
jobs %>% select(Attrition, OverTime, JobLevel, JobSatisfaction) %>% ggpairs(mapping = aes(color=Attrition)) + ggtitle("Attrition vs Over Time, Job Level and Job Satisfaction")

jobs %>% select(MonthlyRate, Age, JobLevel, MonthlyIncome) %>% ggpairs() + ggtitle("Monthly Rate vs Age, Job Level, and Monthly Income")
```

## Over Time Contributing to Attrition
There was a disproportionate amount of overtime in the group of employees that had left. Despite being less than half the number that didn't leave("No"), the group that left were nearly split  in half between overtime and Not
```{r}
summary(jobs$OverTime)
jobs %>% ggplot(aes(x=OverTime)) + geom_histogram(stat="count",mapping = aes(color=Attrition, fill=Attrition)) + ggtitle("Attrition vs Over Time")

```

```{r}
jobs %>% ggplot(aes(x=JobSatisfaction)) + geom_histogram(stat="count",mapping = aes(color=Attrition, fill=Attrition)) + ggtitle("Attrition vs Job Satisfaction")
```


## Data Transformations
Scaling the numeric variables allows us to compare each row to the variation in the overall dataset. We did this by subtracting the mean of that field, then dividing by the standard deviation.

```{r}
jobs<-originalJobs
dummyBoolean <- function(boolean) { 
bool <- 0
if(grepl('Yes', boolean,ignore.case = TRUE))
{bool <-1} 
bool
}
transformFields <- function(dataset){
  dummied_overTime <- dataset %>% mutate(OverTimeDummy =sapply(OverTime, dummyBoolean,simplify = TRUE))
  scaledYear <- dummied_overTime %>% mutate(scaledYears = (YearsInCurrentRole - mean(YearsInCurrentRole)) / sd(YearsInCurrentRole))
  scaledJobs <- scaledYear %>% mutate(scaledJobLevel = (JobLevel - mean(JobLevel)) / sd(JobLevel))
  scaledSatis <- scaledJobs %>% mutate(scaledJobSatisfaction = (JobSatisfaction - mean(JobSatisfaction)) / sd(JobSatisfaction))
  scaledMI <- scaledSatis %>% mutate(scaledMI = (MonthlyIncome - mean(MonthlyIncome)) / sd(MonthlyIncome))
  scaledA <- scaledMI %>% mutate(scaledAge = (Age - mean(Age)) / sd(Age))
  scaledA
}

jobs <- transformFields(jobs)
```

## Comparison of Scaled Variables
Scaling the variables allowed us to compare variable that had responses 1-4 (Job Satisfaction) to something with a scale of 18-60 (Age)
```{r}
jobs %>% select(Attrition, OverTime, scaledJobLevel, scaledJobSatisfaction) %>% ggpairs(mapping = aes(color=Attrition)) + ggtitle("Attrition vs Over Time, Scaled Job Level and Job Satisfaction")
jobs %>% select(MonthlyRate,scaledAge,scaledJobLevel, scaledJobSatisfaction) %>% ggpairs() + ggtitle("Monthly Rate vs Scaled Age, Job Level and Job Satisfaction")


```

## Using KNN to Predict Attrition

We will use the K-Nearest Neighbors model to evaluate datapoints 'close' to what we are trying to predict. Using the scaled values allows us to make this better reflect the variance within our dataset.

```{r}

#Identify the best k
#Set Split percentages for train and test sets
set.seed(10)
splitPerc = .8

#loop through values of k to find best model on 100 generated train/test combos
iterations = 50
numks = 80

masterSpec = matrix(nrow = iterations, ncol = numks)
masterSensitivity = matrix(nrow = iterations, ncol = numks)


fieldToTest = c("scaledJobLevel","OverTimeDummy", "scaledJobSatisfaction")

for(j in 1:iterations)
{
  trainIndices = sample(1:dim(jobs)[1],round(splitPerc * dim(jobs)[1]))
  train = jobs[trainIndices,]
  test = jobs[-trainIndices,]
  for(i in 1:numks)
  {
    classifications = knn(train[,fieldToTest],test[, fieldToTest],train$Attrition, prob = TRUE, k = i)
    CM = confusionMatrix(table(classifications,test$Attrition))
    masterSensitivity[j,i] = CM$byClass[1]
    masterSpec[j,i] = CM$byClass[2]
  }
}
```

## K Values that Maximize Specificity and Sensitivity
We can see that the Sensitivity goes up as we expand K. This is because the 'Positive' class being measured is those who did not leave. This portion of the population is much higher than those that left. While testing, we saw these high k values returning 'No' for all datapoints. 
Specificity is the most important, as we are looking for the employees where attrition is "Yes". Therefore, we prioritized setting the k to maximize the True Negative rate.
```{r}
MeanSen = colMeans(masterSensitivity)
#plot k vs accuracy and identify k with highest accuracy
plot(seq(1,numks,1),MeanSen, type = "l", main="Sensitivity of KNN model vs K value")


MeanSpec = colMeans(masterSpec)
#plot k vs accuracy and identify k with highest accuracy
plot(seq(1,numks,1),MeanSpec, type = "l", main="Specificity of KNN model vs K value")

paste("Highest Sensitivity K Value is ", which.max(MeanSen))
paste("Highest Specificity K Value is ", which.max(MeanSpec))

```


### KNN Using Best K-Value
Using the K Value from the Highest Specificity, we attained specificitys ranging from .26-.27. As we increased the training set size, this went up and we expect higher values in the true test set. 
```{r}
#knn classification using the tuned value of k
set.seed(10)
trainIndices = sample(1:dim(jobs)[1],round(splitPerc * dim(jobs)[1]))
trainjobs = jobs[trainIndices,]
testjobs = jobs[-trainIndices,]
classif <- knn(trainjobs[,fieldToTest],testjobs[,fieldToTest],trainjobs$Attrition, prob=TRUE, k=27)
confusionMatrix(table(classif,testjobs$Attrition))
```

## Predictions for Competition Set
Given the set competition set, we will now apply this KNN model to the that dataset and save it for running against the correct values later.
Given that as the split of our training set went up our specificity increased. We expect to see slightly higher specificty in this set. 

```{r}
competitionAttrition = read.csv("CaseStudy2CompSet No Attrition.csv",header = TRUE)

  compdummied_overTime <- competitionAttrition %>% mutate(OverTimeDummy =sapply(OverTime, dummyBoolean,simplify = TRUE))
  compscaledYear <- compdummied_overTime %>% mutate(scaledYears = (YearsInCurrentRole - mean(YearsInCurrentRole)) / sd(YearsInCurrentRole))
  compscaledJobs <- compscaledYear %>% mutate(scaledJobLevel = (JobLevel - mean(JobLevel)) / sd(JobLevel))
  compscaledSatis <- compscaledJobs %>% mutate(scaledJobSatisfaction = (JobSatisfaction - mean(JobSatisfaction)) / sd(JobSatisfaction))
  compscaledA <- compscaledSatis %>% mutate(scaledAge = (Age - mean(Age)) / sd(Age))
scaledCompetitionAttrition = compscaledA

classif <- knn(jobs[,fieldToTest],scaledCompetitionAttrition[,fieldToTest],jobs$Attrition, prob=TRUE, k=27)

datafromeWithClass <- data.frame(scaledCompetitionAttrition$ID,data.frame(classif))


  write.csv(datafromeWithClass, file="Case2PredictionsLayne AttritionTEST.csv",row.names = FALSE)



```






```{r}
summary(jobs$MonthlyRate)
jobs %>% ggplot(aes(x=MonthlyRate)) + geom_histogram(color="black", fill="white",bins = 30) + ggtitle("Salary Distribution")
```


## Testing the Linear Model over many Root Mean Squares
Our mean of running this linear regression model 1000 times to compare models with different scaled and dummy coded variables. The highest correlated variables to Montly Rate did not always result in a larger 
```{r}
numMSPEs = 1000
RMSEHolderModel1 = numeric(numMSPEs)

for (i in 1:numMSPEs)
{
  TrainObs = sample(seq(1,dim(jobs)[1]),round(.8*dim(jobs)[1]),replace = FALSE)
  jobsTrain = jobs[TrainObs,]
  jobsTest = jobs[-TrainObs,]
  Model1_fit = lm(MonthlyRate ~ scaledAge + scaledJobLevel +scaledJobSatisfaction, data=jobsTrain)
  Model1_Preds = predict(Model1_fit, newdata = jobsTest)
  
  #MSPE Model 1
  RMSE = sqrt(mean((Model1_Preds - jobsTest$MonthlyRate)^2))
  RMSEHolderModel1[i] = RMSE
  
}
mean(RMSEHolderModel1)
```

## Predicting the Competition DataSet

We will now export the competition dataset for predictions

```{r}

competitionSalary = read.csv("CaseStudy2CompSet No Salary.csv",header = TRUE)
  compSalarydummied_overTime <- competitionSalary %>% mutate(OverTimeDummy =sapply(OverTime, dummyBoolean,simplify = TRUE))
  compSalaryscaledYear <- compSalarydummied_overTime %>% mutate(scaledYears = (YearsInCurrentRole - mean(YearsInCurrentRole)) / sd(YearsInCurrentRole))
  compSalaryscaledJobs <- compSalaryscaledYear %>% mutate(scaledJobLevel = (JobLevel - mean(JobLevel)) / sd(JobLevel))
  compSalaryscaledSatis <- compSalaryscaledJobs %>% mutate(scaledJobSatisfaction = (JobSatisfaction - mean(JobSatisfaction)) / sd(JobSatisfaction))
  compSalaryscaledA <- compSalaryscaledSatis %>% mutate(scaledAge = (Age - mean(Age)) / sd(Age))
scaledCompetitionSalary = compSalaryscaledA

  Model_fit = lm(MonthlyRate ~ scaledAge + scaledJobLevel + scaledJobSatisfaction , data=jobs)
  Competition_Preds = predict(Model_fit,newdata = scaledCompetitionSalary)
  dfpreds <- data.frame(Competition_Preds)
  datafromeWithPred<- data.frame(scaledCompetitionSalary$ID,dfpreds)
  write.csv(datafromeWithPred, file = "Case2PredictionsLayne SalaryTEST.csv", row.names = FALSE, na="")
```



```{r}

```